{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8597ff85",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87633a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MANIFEST = \"datasets/vivos/train_manifest.json\"\n",
    "TEST_MANIFEST = \"datasets/vivos/test_manifest.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a364c89",
   "metadata": {},
   "source": [
    "## Preparing the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eee219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VOCAB_SIZE = 128  # can be any value above 29\n",
    "TOKENIZER_TYPE = \"spe\"  # can be wpe or spe\n",
    "SPE_TYPE = \"bpe\"  # can be bpe or unigram\n",
    "\n",
    "# ------------------------------------------------------------------- #\n",
    "!rm -r tokenizers/\n",
    "\n",
    "if not os.path.exists(\"tokenizers\"):\n",
    "  os.makedirs(\"tokenizers\")\n",
    "\n",
    "!python scripts/process_asr_text_tokenizer.py \\\n",
    "   --manifest=$TRAIN_MANIFEST \\\n",
    "   --data_root=\"tokenizers\" \\\n",
    "   --tokenizer=$TOKENIZER_TYPE \\\n",
    "   --spe_type=$SPE_TYPE \\\n",
    "   --no_lower_case \\\n",
    "   --log \\\n",
    "   --vocab_size=$VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a456e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer path\n",
    "if TOKENIZER_TYPE == 'spe':\n",
    "  TOKENIZER = os.path.join(\"tokenizers\", f\"tokenizer_spe_{SPE_TYPE}_v{VOCAB_SIZE}\")\n",
    "  TOKENIZER_TYPE_CFG = \"bpe\"\n",
    "else:\n",
    "  TOKENIZER = os.path.join(\"tokenizers\", f\"tokenizer_wpe_v{VOCAB_SIZE}\")\n",
    "  TOKENIZER_TYPE_CFG = \"wpe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa97b9d",
   "metadata": {},
   "source": [
    "# Load model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2373eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "config = OmegaConf.load(\"configs/contextnet_rnnt.yaml\")\n",
    "# config = OmegaConf.load(\"configs/fast-conformer_transducer_bpe.yaml\")\n",
    "config.model.train_ds.manifest_filepath = TRAIN_MANIFEST\n",
    "config.model.validation_ds.manifest_filepath = TEST_MANIFEST\n",
    "config.model.test_ds.manifest_filepath = TEST_MANIFEST\n",
    "\n",
    "config.model.tokenizer.dir = TOKENIZER\n",
    "config.model.tokenizer.type = TOKENIZER_TYPE_CFG\n",
    "\n",
    "# Finally, let's remove logging of samples and the warmup since the dataset is small (similar to CTC models)\n",
    "config.model.log_prediction = False\n",
    "config.model.optim.sched.warmup_steps = None\n",
    "\n",
    "config.model.spec_augment.freq_masks = 0\n",
    "config.model.spec_augment.time_masks = 0\n",
    "\n",
    "config.model.encoder.jasper = config.model.encoder.jasper[:5]\n",
    "config.model.encoder.jasper[-1].filters = '${model.model_defaults.enc_hidden}'\n",
    "# config.model.encoder.n_layers = 6\n",
    "# config.model.encoder.d_model = 176\n",
    "# config.model.encoder.n_heads = 1\n",
    "# config.model.train_ds.max_duration = 5\n",
    "# config.model.encoder.conv_kernel_size = 17\n",
    "\n",
    "# Two lines to enable the fused batch step\n",
    "config.model.joint.fuse_loss_wer = True\n",
    "config.model.joint.fused_batch_size = 16  # this can be any value (preferably less than model.*_ds.batch_size)\n",
    "\n",
    "# We will also reduce the hidden dimension of the joint and the prediction networks to preserve some memory\n",
    "config.model.model_defaults.pred_hidden = 64\n",
    "config.model.model_defaults.joint_hidden = 64\n",
    "config.model.model_defaults.filters = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ca193",
   "metadata": {},
   "source": [
    "## Initialize a Transducer ASR Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  accelerator = 'gpu'\n",
    "else:\n",
    "  accelerator = 'gpu'\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialize a Trainer for the Transducer model\n",
    "trainer = Trainer(devices=1, accelerator=accelerator, max_epochs=EPOCHS,\n",
    "                  enable_checkpointing=False, logger=False,\n",
    "                  log_every_n_steps=10, check_val_every_n_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "model = nemo_asr.models.EncDecRNNTBPEModel(cfg=config.model, trainer=trainer)\n",
    "model.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bb3a1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare NeMo's Experiment manager to handle checkpoint saving and logging for us\n",
    "from nemo.utils import exp_manager\n",
    "\n",
    "# Environment variable generally used for multi-node multi-gpu training.\n",
    "# In notebook environments, this flag is unnecessary and can cause logs of multiple training runs to overwrite each other.\n",
    "os.environ.pop('NEMO_EXPM_VERSION', None)\n",
    "\n",
    "exp_config = exp_manager.ExpManagerConfig(\n",
    "    exp_dir=f'experiments/',\n",
    "    name=f\"Transducer-Model\",\n",
    "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "        monitor=\"val_wer\",\n",
    "        mode=\"min\",\n",
    "        always_save_nemo=True,\n",
    "        save_best_model=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "exp_config = OmegaConf.structured(exp_config)\n",
    "\n",
    "logdir = exp_manager.exp_manager(trainer, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de69dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google import colab\n",
    "  COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "  COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir /content/experiments/Transducer-Model/\n",
    "else:\n",
    "  print(\"To use TensorBoard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ddfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release resources prior to training\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "if accelerator == 'gpu':\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2efd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-19 21:20:28 mixins:181] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-19 21:20:28 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: datasets/vivos/train_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    max_duration: 17.125\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-07-19 21:20:28 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: datasets/vivos/test_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-07-19 21:20:28 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: datasets/vivos/test_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-19 21:20:28 features:305] PADDING: 0\n",
      "[NeMo I 2025-07-19 21:20:28 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-07-19 21:20:28 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-07-19 21:20:28 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-07-19 21:20:29 save_restore_connector:275] Model EncDecRNNTBPEModel was successfully restored from /home/ubuntu/nvidia_nemo/tutorials/asr/experiments/vpb_asr_fastconformer_transducer_bpe/2025-07-19_15-59-19/checkpoints/vpb_asr_fastconformer_transducer_bpe.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "model = nemo_asr.models.EncDecRNNTBPEModel.restore_from(\"experiments/vpb_asr_fastconformer_transducer_bpe/2025-07-19_15-59-19/checkpoints/vpb_asr_fastconformer_transducer_bpe.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9749a7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 100%|██████████| 1/1 [00:00<00:00, 22.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribe text:  [Hypothesis(score=483.499755859375, y_sequence=tensor([ 11,   3,  18,   4,  16,  40,  10,   1, 117,  31,  10,  23,  16, 109,\n",
      "          2,   1, 122, 114,   3,  35,   1, 111,   3,   1,   8,  44,  16,  46,\n",
      "         24,   1, 116, 118,  10,  23], device='cuda:0'), text='tiếng cọc kịch cận lại của ối những cớp sách', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 100%|██████████| 1/1 [00:00<00:00, 30.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribe text:  [Hypothesis(score=483.499755859375, y_sequence=tensor([ 11,   3,  18,   4,  16,  40,  10,   1, 117,  31,  10,  23,  16, 109,\n",
      "          2,   1, 122, 114,   3,  35,   1, 111,   3,   1,   8,  44,  16,  46,\n",
      "         24,   1, 116, 118,  10,  23], device='cuda:0'), text='tiếng cọc kịch cận lại của ối những cớp sách', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = model.transcribe('/home/ubuntu/.cache/kagglehub/datasets/kynthesis/vivos-vietnamese-speech-corpus-for-asr/versions/1/vivos/test/waves/VIVOSDEV01/VIVOSDEV01_R002.wav')\n",
    "print(\"Transcribe text: \", output)\n",
    "\n",
    "utput = model.transcribe('datasets/vivos/test/augumented_8k_waves/VIVOSDEV01/VIVOSDEV01_R002.wav')\n",
    "print(\"Transcribe text: \", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
